# -*- coding: utf-8 -*-
"""Drug_reviews_LTSM_Collab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VF3p92hobOmSboEfJXoGTtf_MchnPaeu
"""

# Imports 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import log_loss, make_scorer, confusion_matrix
import seaborn as sns
from statsmodels.stats.proportion import proportions_chisquare
from scipy.stats import chisquare
import pickle
from bs4 import BeautifulSoup
from collections import defaultdict
import requests
from statsmodels.stats.multitest import fdrcorrection_twostage
### Keras 
from keras.preprocessing import sequence
from keras.preprocessing.text import Tokenizer
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.models import Sequential
from keras.layers import Embedding,SpatialDropout1D,LSTM,Dense
import os
import numpy.random as rand
from itertools import islice
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.svm import SVC
import seaborn as sns
import math
import sklearn.datasets as datasets
# import sklearn.cross_validation as cv
import sklearn.model_selection as cv
from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.metrics import log_loss, make_scorer, confusion_matrix
from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
import datetime
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
from keras.layers import Embedding,SpatialDropout1D,LSTM,Dense
current_palette = sns.color_palette()
sns.palplot(current_palette)

from pylab import rcParams
rcParams['figure.figsize'] = (12, 8)

import unicodedata
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
from nltk.stem.porter import PorterStemmer
from nltk.stem.snowball import SnowballStemmer
import re

# Creating the Target Label 
def create_rating_label(df):
    if df['rating'] >= 7: 
        return 'high'
    elif (df['rating'] < 4):
        return 'low'
    else :
        return 'neutral'

# Creating the Text Label 
def clean_text(text):
    """
        text: a string
        
        return: modified initial string
    """
    REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
    BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
    STOPWORDS = set(stopwords.words('english'))
    text = text.lower() # lowercase text
    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.
    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. 
    text = text.replace('x', '')
#    text = re.sub(r'\W+', '', text)
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text
    return text





## Tokenize the review data 

def Tokenize_data(df_train,df_test):
    MAX_NB_WORDS = 3000
    MAX_SEQUENCE_LENGTH = 100
    EMBEDDING_DIM = 140
    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~``', lower=True)
    tokenizer.fit_on_texts(df_train['review'].values)
    word_index = tokenizer.word_index
    X_train = tokenizer.texts_to_sequences(df_train['review'].values)
    X_train = sequence.pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)
    X_test = tokenizer.texts_to_sequences(df_test['review'].values)
    X_test = sequence.pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)
    Y_train = pd.get_dummies(df_train['label']).values
    Y_test = pd.get_dummies(df_test['label']).values

    return X_train,X_train,Y_train,Y_test

# Reading the data and Prreprocessing and cleaning 
df_train = pd.read_table('drugsComTrain_raw.tsv')
df_test = pd.read_table('drugsComTest_raw.tsv')
df_train['label']=df_train.apply(create_rating_label, axis=1)
df_test['label']=df_test.apply(create_rating_label, axis=1)
df_train = df_train.reset_index(drop=True)
df_train['review'] = df_train['review'].apply(clean_text)
df_train['review'] = df_train['review'].str.replace('\d+', '')
X_train,X_train,Y_train,Y_test=Tokenize_data(df_train,df_test)
###### Creating the Model #########
MAX_NB_WORDS = 3000
MAX_SEQUENCE_LENGTH = 100
EMBEDDING_DIM = 140
model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))
model.add(LSTM(100,return_sequences=True))
model.add(LSTM(50,return_sequences=False))
model.add(Dense(3, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

epochs = 3
batch_size = 32

history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])

accr = model.evaluate(X_test,Y_test)

